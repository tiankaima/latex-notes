<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="少年班学院  马天开  PB21000030" />
  <meta name="dcterms.date" content="2022-05-24" />
  <title>针对ACG风格图像识别算法的一种改进和应用</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">针对ACG风格图像识别算法的一种改进和应用</h1>
<p class="author">少年班学院  马天开  PB21000030</p>
<p class="date">2022-05-24</p>
</header>
<h1 id="科学技术原理">科学技术原理</h1>
<p>图像识别算法在近一个世纪以来都是深度学习的重要应用之一，在疫情期间也在部分场景中发挥了重要用途（比如
检查是否佩戴口罩、识别人脸及对应体温等）。</p>
<p>ACG风格是一种独特的绘画/美术风格，与现实照片相比，一般具有轮廓加粗、高对比度等特征。部分适用于现实照片的图像处理工具在ACG风格图片上可能出现准确率低、耗时更高的情况（尽管这类画风在理论上其实更容易被算法处理）。</p>
<figure>
<img src="img/ACG.jpg" alt="ACG风格图像" />
<figcaption aria-hidden="true">ACG风格图像</figcaption>
</figure>
<p>我们从被广泛使用的<em>Yolov5</em>算法开始，本着重造轮子的精神，对其训练和检测算法进行重构，并给出改进后算法的一个可行应用，以及应用背后的意义。</p>
<h2 id="yolov5模型简述"><em>Yolov5</em>模型简述</h2>
<p><em>Yolov5</em>模型相较于上一代很相似，还是分为输入端、Backbone、Neck、Prediction四个部分。</p>
<figure>
<img src="img/yolov5.jpg" alt="Yolov5模型" />
<figcaption aria-hidden="true">Yolov5模型</figcaption>
</figure>
<p>其中，在本项目中，相较于<em>Yolov4</em>，提升最大的地方在于自适应锚框计算和图片缩放上。</p>
<p>另外，在<em>dataset.py</em>中，<em>Yolov5</em>给出了新的缩放算法，减少了缩放图片时两侧的黑边，将推理速度提升了37%。</p>
<h2 id="改进思路">改进思路</h2>
<p>针对ACG这种不算复杂的图像，<em>Yolov5s</em>能在保证精度的前提下最大化提升推理推理速度，在此基础上，我们再略微提升一点精度，便可达到更大的平衡。</p>
<p>在模型训练上，加入了<em>Mosaic</em>数据增强方式，提升了鲁棒性。同时删除了模糊增强的部分，改为GrayScale的方式，提升颜色分块在训练结果中的作用。</p>
<pre><code>    if self.augment:
        img, labels = self.albumentations(img, labels)</code></pre>
<p>同时，上文提到的<em>letterbox</em>函数也可以进一步优化（前后对比发现，过度缩放会带来训练精度的显著降低），改为以下逻辑：</p>
<pre><code>    shape = im.shape[:2]
    if isinstance(new_shape, int):
        new_shape = (new_shape, new_shape)

    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    if not scaleup:
        r = min(r, 1.0)

    ratio = r, r
    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]
    if auto:
        dw, dh = np.mod(dw, stride), np.mod(dh, stride)</code></pre>
<h2 id="应用思路">应用思路</h2>
<p>经过改进后，在样本量为300的（采集自Pixiv的图片），针对识别<strong>眼镜</strong>这一主体，精度可以维持在<span
class="math inline">97.3%</span>（训练集大小：<span
class="math inline">20</span>，<span
class="math inline"><em>e</em><em>p</em><em>o</em><em>c</em><em>h</em> = 40</span>）</p>
<p>为更直观地展示改进后算法的能力，我选择了一个对延迟、精度都很敏感的场景：音乐游戏。（简单来说就是需要在指定时间，根据屏幕提示，按下特定按键的场景）</p>
<p>为此，适配了<em>Windows</em>平台下模拟按键的脚本，并完成了从识别到点击的拟合逻辑(多帧线性插值)。</p>
<p>项目演示地址：<a href="https://www.bilibili.com/video/BV1NU4y127rH/"
class="uri">https://www.bilibili.com/video/BV1NU4y127rH/</a></p>
<figure>
<img src="img/qrcode.png" alt="演示地址" />
<figcaption aria-hidden="true">演示地址</figcaption>
</figure>
<h1 id="设计方案">设计方案</h1>
<p>复现项目的主要过程为：</p>
<ul>
<li><p>提取数据，并将其转换为受<em>Yolov5</em>支持的格式，类似的工作已经有机构做过非常方便的工具。<br />
参考<a href="http://roboflow.com/"
class="uri">http://roboflow.com/</a></p></li>
<li><p>训练数据，并使用<em>Yolov5</em>模型进行推理，对应的文件为<em>train.py</em>，其中大部分代码为我重构的部分，小部分直接引用了<em>Yolov5</em>源码。</p></li>
<li><p>打开游戏，同时打开脚本，此时脚本将会自动识别屏幕上出现的物体，自动帮助按下<em>f</em>和<em>j</em>键，以完成游戏。</p></li>
</ul>
<p>按照实际使用的顺序，程序的大致结构为：</p>
<ul>
<li><p><em>\utils</em>，此目录包含训练时绝大部分所需要使用的script，其中较为突出的时<em>activations.py</em>作为激活函数，<em>loss.py</em>评估损失函数</p></li>
<li><p><em>\models</em>，此目录为<em>Yolov5</em>拷贝的内容，用于导出训练结果。</p></li>
<li><p><em>train.py</em>用于训练新模型，<em>key_win.py</em>用于模拟Windows下键盘输入。</p></li>
</ul>
<h1 id="创新性描述">创新性描述</h1>
<p>目录下除<em>\models</em>，以及<em>utils/loggers</em>为<em>Yolov5</em>拷贝的内容外，其余部分都是我自己独立完成或重构的代码。</p>
<p>在重构代码时，以不抄袭地复现原有功能为基础，加深了我对<em>Yolov5</em>的理解。</p>
<p>项目的优点：</p>
<ul>
<li><p>针对ACG风格图像，对识别算法进行了一次改进</p></li>
<li><p>在应用上，能够直观地反映新算法带来的精度和效率的能力</p></li>
<li><p>更加直观地反映深度学习模型的能力</p></li>
</ul>
<h1 id="运行方法和参数设置">运行方法和参数设置</h1>
<p>要复现本实验的结果，首先要整理一套完整的数据。为方便起见，可以参考<em>fetch_data.py</em>中的代码，这里面包含了我本次使用的数据集。</p>
<p>接下来是训练，可以用以下命令调用<em>train.py</em>：</p>
<pre><code>    python train.py --img 640 --batch 4 --epochs 100 --data /path/to/data.yaml --weights yolov5s.pt</code></pre>
<p>在训练推理结束时，可以用以下命令简单评估训练结果：</p>
<pre><code>    python .\detect.py --weights /path/to/best.pt --source /path/to/img</code></pre>
<p>接下来，打开脚本和游戏（先后顺序无关，脚本会自动采集第一块屏幕的数据），在脚本识别到对应物体时，便会自动按下按键。</p>
<h1 id="学习心得和收获">学习心得和收获</h1>
<p>在本次课程之前，我对Python一直处在一个自己摸索、时常碰壁的学习路上。在最开始使用的时候更为严重，平均下来写代码和排查错误的时间大概要对半分。也不能很好地利用报错信息来排查错误，导致我最开始使用Python的体验相当不好，甚至我一度怀疑这样一门语言对效率的提升究竟有多大。</p>
<p>在本次课程之后，尤其是从课程一开始我就在搞的这个项目，逐渐锻炼了我排查错误、调试以及熟练查文档的能力。现在能达到几乎写C++效率的3 5倍，可以说真正地把Python当作一个有趣的工具来使用。也希望它能在日后的学习/工作中更大地发挥作用。</p>
<p>具体一点讲的话，在本次项目中收获最大的应该要数Anaconda和Pycharm的使用了。有趣的工具总是能带来更有趣的体验，在熟练使用之后，会比单纯使用VSCode的体验和速度更上一层楼。</p>
<p>同时，也是最大的帮助——更熟练了<em>pyTorch</em>的使用，在日常使用<em>tensorflow</em>的基础上，另一种工具的学习总是显得相得益彰。</p>
<h1 id="参考资料">参考资料</h1>
<p>在目标检测上，参考了<a href="https://zhuanlan.zhihu.com/p/454472695"
class="uri">https://zhuanlan.zhihu.com/p/454472695</a>
里面提到的<em>Yolo</em>的检测方法。</p>
<p>YoloV5 模型的学习和理解参考了<a
href="https://zhuanlan.zhihu.com/p/143747206"
class="uri">https://zhuanlan.zhihu.com/p/143747206</a>及其下面的一些的文章，我对于模型的了解基本建立在这几篇文章和源码的基础上，在此表示感谢。</p>
<p>同时，参考了以下文献：</p>
<p>YOLOv3: An Incremental Improvement, Joseph Redmon &amp; Ali Farhadi,
arXiv:1804.02767v1 [cs.CV] 8 Apr 2018</p>
<p>YOLOv4: Optimal Speed and Accuracy of Object Detection Alexey
Bochkovskiy &amp; Chien-Yao Wang &amp; Hong-Yuan Mark Liao</p>
<p>在此一并表示感谢。</p>
</body>
</html>
